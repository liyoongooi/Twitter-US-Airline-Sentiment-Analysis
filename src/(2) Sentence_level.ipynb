{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Sentence-level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\60175\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, RobertaTokenizer, RobertaForSequenceClassification, pipeline\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Loading raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/(A) data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.set_index('tweet_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Dropping unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(['airline_sentiment_gold', 'name', 'negativereason_gold', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.rename(columns={'airline_sentiment': 'given_sentiment',\n",
    "                         'airline_sentiment_confidence': 'given_sentiment_confidence',\n",
    "                         'negativereason': 'given_negative_reason',\n",
    "                         'negativereason_confidence': 'given_negative_reason_confidence',\n",
    "                         'text': 'tweet'\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Dropping duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Dropping index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['given_sentiment'] = df_clean['given_sentiment'].astype('string')\n",
    "df_clean['given_negative_reason'] = df_clean['given_negative_reason'].astype('string')\n",
    "df_clean['airline'] = df_clean['airline'].astype('string')\n",
    "df_clean['tweet'] = df_clean['tweet'].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Preserving letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['tweet'] = df_clean['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Decapitilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['tweet'] = df_clean['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['flight', 'will'])\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sia_sentiment(text):\n",
    "    scores = sia.polarity_scores(text) \n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment = 'positive'\n",
    "        confidence = scores['pos']\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment = 'negative'\n",
    "        confidence = scores['neg']\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "        confidence = scores['neu']  \n",
    "    return pd.Series([sentiment, confidence])  \n",
    "\n",
    "df_clean[['sia_sentiment', 'sia_sentiment_confidence']] = df_clean['tweet'].apply(sia_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "bert_sentiment_model = pipeline(\"sentiment-analysis\", model=bert_model, tokenizer=bert_tokenizer)\n",
    "bert_label_mapping = {\n",
    "    'POS': 'positive',\n",
    "    'NEG': 'negative',\n",
    "    'NEU': 'neutral'\n",
    "}\n",
    "\n",
    "def bert_sentiment(text):\n",
    "    result = bert_sentiment_model(text)[0]\n",
    "    sentiment = bert_label_mapping.get(result['label'], result['label']).lower()\n",
    "    confidence = result['score']\n",
    "    return pd.Series([sentiment, confidence])\n",
    "\n",
    "df_clean[['bert_sentiment', 'bert_sentiment_confidence']] = df_clean['tweet'].apply(bert_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "roberta_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(roberta_model_name)\n",
    "roberta_sentiment_model = pipeline(\"sentiment-analysis\", model=roberta_model, tokenizer=roberta_tokenizer)\n",
    "\n",
    "def roberta_sentiment(text):\n",
    "    result = roberta_sentiment_model(text)[0]\n",
    "    sentiment = result['label']\n",
    "    confidence = result['score']\n",
    "    return pd.Series([sentiment, confidence])\n",
    "\n",
    "df_clean[['roberta_sentiment', 'roberta_sentiment_confidence']] = df_clean['tweet'].apply(roberta_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Multilingual distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_model_name)\n",
    "distilbert_model = AutoModelForSequenceClassification.from_pretrained(distilbert_model_name)\n",
    "distilbert_sentiment_model = pipeline(\"sentiment-analysis\", model=distilbert_model, tokenizer=distilbert_tokenizer)\n",
    "\n",
    "def distilbert_sentiment(text):\n",
    "    result = distilbert_sentiment_model(text)[0]\n",
    "    sentiment = result['label']\n",
    "    confidence = result['score']\n",
    "    return pd.Series([sentiment, confidence])\n",
    "\n",
    "df_clean[['distilbert_sentiment', 'distilbert_sentiment_confidence']] = df_clean['tweet'].apply(distilbert_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Comparison of Sentiment Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_sentiment_confidence</th>\n",
       "      <th>sia_sentiment_confidence</th>\n",
       "      <th>bert_sentiment_confidence</th>\n",
       "      <th>roberta_sentiment_confidence</th>\n",
       "      <th>distilbert_sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14584.000000</td>\n",
       "      <td>14584.000000</td>\n",
       "      <td>14584.000000</td>\n",
       "      <td>14584.000000</td>\n",
       "      <td>14584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.899786</td>\n",
       "      <td>0.482629</td>\n",
       "      <td>0.874637</td>\n",
       "      <td>0.779011</td>\n",
       "      <td>0.575477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.163025</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.135260</td>\n",
       "      <td>0.139074</td>\n",
       "      <td>0.152964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351074</td>\n",
       "      <td>0.347661</td>\n",
       "      <td>0.335754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.257750</td>\n",
       "      <td>0.816801</td>\n",
       "      <td>0.676624</td>\n",
       "      <td>0.455013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.940087</td>\n",
       "      <td>0.812208</td>\n",
       "      <td>0.531459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.971652</td>\n",
       "      <td>0.894637</td>\n",
       "      <td>0.672984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993168</td>\n",
       "      <td>0.989277</td>\n",
       "      <td>0.986638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       given_sentiment_confidence  sia_sentiment_confidence  \\\n",
       "count                14584.000000              14584.000000   \n",
       "mean                     0.899786                  0.482629   \n",
       "std                      0.163025                  0.288256   \n",
       "min                      0.335000                  0.000000   \n",
       "25%                      0.692200                  0.257750   \n",
       "50%                      1.000000                  0.384000   \n",
       "75%                      1.000000                  0.636000   \n",
       "max                      1.000000                  1.000000   \n",
       "\n",
       "       bert_sentiment_confidence  roberta_sentiment_confidence  \\\n",
       "count               14584.000000                  14584.000000   \n",
       "mean                    0.874637                      0.779011   \n",
       "std                     0.135260                      0.139074   \n",
       "min                     0.351074                      0.347661   \n",
       "25%                     0.816801                      0.676624   \n",
       "50%                     0.940087                      0.812208   \n",
       "75%                     0.971652                      0.894637   \n",
       "max                     0.993168                      0.989277   \n",
       "\n",
       "       distilbert_sentiment_confidence  \n",
       "count                     14584.000000  \n",
       "mean                          0.575477  \n",
       "std                           0.152964  \n",
       "min                           0.335754  \n",
       "25%                           0.455013  \n",
       "50%                           0.531459  \n",
       "75%                           0.672984  \n",
       "max                           0.986638  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[['given_sentiment_confidence', 'sia_sentiment_confidence', 'bert_sentiment_confidence', 'roberta_sentiment_confidence', 'distilbert_sentiment_confidence']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> None of the experimented approach yields a higher mean sentiment confidence than the given labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> BERT's performance is quite close to the given model (assuming it's from a model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Ensemble (0.5*Given + 0.3*BERT + 0.2*roBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'given': 0.5,\n",
    "    'bert': 0.3,\n",
    "    'roberta': 0.2\n",
    "}\n",
    "\n",
    "def weighted_ensemble(row):\n",
    "    weighted_confidence = {\n",
    "        'positive': 0,\n",
    "        'neutral': 0,\n",
    "        'negative': 0\n",
    "    }\n",
    "    \n",
    "    weighted_confidence[row['given_sentiment']] += weights['given'] * row['given_sentiment_confidence']\n",
    "    weighted_confidence[row['bert_sentiment']] += weights['bert'] * row['bert_sentiment_confidence']\n",
    "    weighted_confidence[row['roberta_sentiment']] += weights['roberta'] * row['roberta_sentiment_confidence']\n",
    "    final_sentiment = max(weighted_confidence, key=weighted_confidence.get)\n",
    "    confidences = {\n",
    "        'given': row['given_sentiment_confidence'] if row['given_sentiment'] == final_sentiment else 0,\n",
    "        'bert': row['bert_sentiment_confidence'] if row['bert_sentiment'] == final_sentiment else 0,\n",
    "        'roberta': row['roberta_sentiment_confidence'] if row['roberta_sentiment'] == final_sentiment else 0\n",
    "    }\n",
    "    final_confidence = max(confidences.values())\n",
    "\n",
    "    return pd.Series([final_sentiment, final_confidence])\n",
    "\n",
    "df_clean[['ensemble_sentiment', 'ensemble_sentiment_confidence']] = df_clean.apply(weighted_ensemble, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_sentiment_confidence</th>\n",
       "      <th>ensemble_sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14584.000000</td>\n",
       "      <td>14584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.899786</td>\n",
       "      <td>0.959044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.163025</td>\n",
       "      <td>0.090851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.350300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.971787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       given_sentiment_confidence  ensemble_sentiment_confidence\n",
       "count                14584.000000                   14584.000000\n",
       "mean                     0.899786                       0.959044\n",
       "std                      0.163025                       0.090851\n",
       "min                      0.335000                       0.350300\n",
       "25%                      0.692200                       0.971787\n",
       "50%                      1.000000                       1.000000\n",
       "75%                      1.000000                       1.000000\n",
       "max                      1.000000                       1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[['given_sentiment_confidence', 'ensemble_sentiment_confidence']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>given_sentiment</th>\n",
       "      <th>ensemble_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica plus youve added commercial experience tacky</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica didnt today must mean need take another trip</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>virginamerica yes nearly every time fly vx ear worm wont go away</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>virginamerica really missed prime opportunity men without hat parody httpstcomwpggrezp</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>virginamerica know suicide second leading cause death among teen</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14509</th>\n",
       "      <td>americanair cant ahold aadvantage reservation need ticket reservation cancelled flight soon help</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14531</th>\n",
       "      <td>americanair ill play ear know best buy chewey oatmeal cooky customer care folk</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14539</th>\n",
       "      <td>americanair shannonbloom wheres dm wheres voucher who paying cab car back jfk tomorrow</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576</th>\n",
       "      <td>americanair tilleymonsta george doesnt look good please follow link start refund process httptcogrsdl</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14579</th>\n",
       "      <td>americanair thank got different chicago</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1317 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       tweet  \\\n",
       "1                                                 virginamerica plus youve added commercial experience tacky   \n",
       "2                                                 virginamerica didnt today must mean need take another trip   \n",
       "6                                           virginamerica yes nearly every time fly vx ear worm wont go away   \n",
       "7                     virginamerica really missed prime opportunity men without hat parody httpstcomwpggrezp   \n",
       "10                                          virginamerica know suicide second leading cause death among teen   \n",
       "...                                                                                                      ...   \n",
       "14509       americanair cant ahold aadvantage reservation need ticket reservation cancelled flight soon help   \n",
       "14531                         americanair ill play ear know best buy chewey oatmeal cooky customer care folk   \n",
       "14539                 americanair shannonbloom wheres dm wheres voucher who paying cab car back jfk tomorrow   \n",
       "14576  americanair tilleymonsta george doesnt look good please follow link start refund process httptcogrsdl   \n",
       "14579                                                                americanair thank got different chicago   \n",
       "\n",
       "      given_sentiment ensemble_sentiment  \n",
       "1            positive           negative  \n",
       "2             neutral           negative  \n",
       "6            positive           negative  \n",
       "7             neutral           negative  \n",
       "10            neutral           negative  \n",
       "...               ...                ...  \n",
       "14509         neutral           negative  \n",
       "14531        negative            neutral  \n",
       "14539        negative            neutral  \n",
       "14576         neutral           negative  \n",
       "14579        positive            neutral  \n",
       "\n",
       "[1317 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[df_clean['given_sentiment'] != df_clean['ensemble_sentiment']][['tweet', 'given_sentiment', 'ensemble_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Save pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('../data/(B) sentence_level_full_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_clean.copy()\n",
    "df_final.drop(['given_sentiment', 'given_sentiment_confidence', 'sia_sentiment', 'sia_sentiment_confidence',\n",
    "       'bert_sentiment', 'bert_sentiment_confidence', 'roberta_sentiment',\n",
    "       'roberta_sentiment_confidence', 'distilbert_sentiment',\n",
    "       'distilbert_sentiment_confidence'], axis = 1, inplace = True)\n",
    "df_final.to_csv('../data/(C) sentence_level_final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_final[['tweet', 'ensemble_sentiment']]\n",
    "df_class.to_csv('../data/(D) classification.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
